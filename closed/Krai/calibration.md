# MLPerf Inference v3.0 - Krai - Calibration Details

## TFLite/ArmNN

We use quantized TFLite models shared by Google.
We believe that Google used [post-training quantization](https://www.tensorflow.org/lite/performance/post_training_quantization) via the [TFLite converter](https://www.tensorflow.org/lite/models/convert).

## TensorRT

Please refer to NVIDIA's [v2.1 calibration document](https://github.com/mlcommons/inference_results_v2.1/blob/master/closed/NVIDIA/documentation/calibration.md).
